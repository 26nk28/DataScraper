{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from time import sleep\n",
    "import xlwt \n",
    "from xlwt import Workbook \n",
    "  \n",
    "import xlsxwriter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"question_testing111111.txt\",\"w+\")\n",
    "g = open(\"answer_testing111111.txt\",\"w+\")\n",
    "h =  open(\"lawyer_data111.txt\",\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def questions_extract(link,j,i):\n",
    "    \n",
    "\n",
    " \n",
    "\n",
    "        \n",
    "        \n",
    "    url = link\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "    \n",
    "    links_question = soup.findAll(['div','strong','a'] ,\n",
    "                    {'class':\"question-wrapper has-padding-content-block-30 has-negative-sides-30 clearfix\",\n",
    "                      'class':\"heading-3 has-no-top-margin -question text-soft-wrap\"})[i]\n",
    "    link_to_full_question = links_question.a['href']\n",
    "    link_to_full_question\n",
    "    url1 = \"https://answers.justia.com\"+ link_to_full_question\n",
    "    response1 =requests.get(url1)\n",
    "    \n",
    "    soup1 = BeautifulSoup(response1.text,'html.parser')\n",
    "    try:\n",
    "        full_question = soup1.findAll(['div','p'], {'class':\"block question-main-wrap\"})[0]\n",
    "            \n",
    "\n",
    "        second_part = full_question.p.text[2:].strip()\n",
    "\n",
    "        part = soup1.findAll('h1')[0]\n",
    "        first_part = part.text[3:]\n",
    "\n",
    "        total_qn = first_part + second_part\n",
    "        question = str(total_qn+'|')\n",
    "        if question.find('\\n')<len(question):\n",
    "            question = question.replace('\\n','')\n",
    "        \n",
    "        f.write(question)\n",
    "        \n",
    "    except:\n",
    "        part = soup1.findAll('h1')[0]\n",
    "        first_part = part.text[3:]\n",
    "        \n",
    "        total_qn = first_part\n",
    "        question = str(total_qn+'|')\n",
    "        if question.find('\\n')<len(question):\n",
    "            question = question.replace('\\n','')\n",
    "        \n",
    "        f.write(question)\n",
    "        \n",
    "            \n",
    "    time_and_ID = soup.findAll('div',{'class':\"lawyers-who-answered has-no-bottom-margin\", 'class':\"-answer text-soft-wrap\"})[i]\n",
    "    data_profile=soup.findAll('div',{'class':\"lawyers-who-answered has-no-bottom-margin\" ,\"class\":\"lawyer clearfix\"})[i]\n",
    "    profile_id = str(data_profile['data-profile']+'|')\n",
    "    lawyer_name = str(time_and_ID.strong.text.split('answered on')[0].strip()+'|')\n",
    "    time_answered = str(time_and_ID.strong.text.split('answered on')[1].strip()+'|')\n",
    "       \n",
    "    \n",
    "    f.write(time_answered)\n",
    "    f.write(profile_id)\n",
    "    f.write(lawyer_name)\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_and_field(link,j,i):\n",
    "    url = link\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "    \n",
    "    links_question = soup.findAll(['div','strong','a','strong'] ,\n",
    "                    {'class':\"question-wrapper has-padding-content-block-30 has-negative-sides-30 clearfix\",\n",
    "                      'class':\"question-info small-font has-no-top-margin\"})[i]\n",
    "    index_in= links_question.text.find('in')\n",
    "    index_for= links_question.text.find('for')\n",
    "\n",
    "    field = links_question.text[index_in+6:index_for-15] + '|'\n",
    "    area = links_question.text[index_for+5:-37]\n",
    "\n",
    "    f.write(field)\n",
    "    f.write(area)\n",
    "    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def answers_extract(link,j,i):\n",
    "        \n",
    "    \n",
    "        \n",
    "        url = link\n",
    "        print(url)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "        \n",
    "        links_answer_link= soup.findAll(['div','span'],{'class':\"question-wrapper has-padding-content-block-30 has-negative-sides-30 clearfix\",\n",
    "                                           'class':\"lawyers-who-answered has-no-bottom-margin\",\n",
    "                                           'class':\"-answer text-soft-wrap\"})[i]\n",
    "        try:\n",
    "            answer_link = \"https://answers.justia.com\" + links_answer_link.p.strong.a['href']\n",
    "            url_answer = answer_link\n",
    "            response = requests.get(url_answer)\n",
    "            soup_answer = BeautifulSoup(response.text,'html.parser')\n",
    "            soup_answer_found = soup_answer.findAll(['div','p'],{'class':\"answer-detailed-text to-xlarge-font text-soft-wrap\",\n",
    "                                                    'class':\"answer-detailed-text to-xlarge-font\"})[0]\n",
    "                \n",
    "                \n",
    "            time_and_ID = soup.findAll('div',{'class':\"lawyers-who-answered has-no-bottom-margin\", 'class':\"-answer text-soft-wrap\"})[i]\n",
    "            data_profile=soup.findAll('div',{'class':\"lawyers-who-answered has-no-bottom-margin\" ,\"class\":\"lawyer clearfix\"})[i]\n",
    "            profile_id = str(data_profile['data-profile']+'|')\n",
    "            lawyer_name = str(time_and_ID.strong.text.split('answered on')[0].strip())\n",
    "            time_answered = str(time_and_ID.strong.text.split('answered on')[1].strip()+'|')\n",
    "            answer = str(soup_answer_found.text.strip()[3:]+'|')\n",
    "            g.write(answer)\n",
    "            g.write(time_answered)\n",
    "            g.write(profile_id)\n",
    "            g.write(lawyer_name)\n",
    "            g.write(\"\\n\")\n",
    "            \n",
    "                \n",
    "               \n",
    "        except:\n",
    "                \n",
    "            \n",
    "            time_and_ID = soup.findAll('div',{'class':\"lawyers-who-answered has-no-bottom-margin\", 'class':\"-answer text-soft-wrap\"})[i]\n",
    "            data_profile=soup.findAll('div',{'class':\"lawyers-who-answered has-no-bottom-margin\" ,\"class\":\"lawyer clearfix\"})[i]\n",
    "            profile_id = str(data_profile['data-profile']+'|')\n",
    "            lawyer_name = str(time_and_ID.strong.text.split('answered on')[0].strip())\n",
    "            time_answered = str(time_and_ID.strong.text.split('answered on')[1].strip()+'|')\n",
    "            answer = str(links_answer_link.p.span.text+'|')\n",
    "            g.write(answer)\n",
    "            g.write(time_answered)\n",
    "            g.write(profile_id)\n",
    "            g.write(lawyer_name)\n",
    "            g.write(\"\\n\")\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lawyer_data(link,j,i):\n",
    "    url = link\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "        \n",
    "    links_answer_link= soup.findAll(['div','span'],{'class':\"question-wrapper has-padding-content-block-30 has-negative-sides-30 clearfix\",\n",
    "                                           'class':\"lawyers-who-answered has-no-bottom-margin\",\n",
    "                                           'class':\"-answer text-soft-wrap\"})[i]\n",
    "    link_to_lawyer = links_answer_link.a['href']\n",
    "    url1 = link_to_lawyer\n",
    "    print(url1)\n",
    "    response1 = requests.get(url1)\n",
    "    soup1 = BeautifulSoup(response1.text,'html.parser')\n",
    "\n",
    "    lawyer1 = soup1.findAll(['div'],{'class':\"lawyer-coreinfo has-padding-30 has-no-bottom-padding\"})[0]\n",
    "    lawyer_name = lawyer1.h1.text\n",
    "    lawyer_id_link= links_answer_link.a['href']\n",
    "\n",
    "    links_answer_link1 = soup.findAll(['div','span'],{'class':\"question-wrapper has-padding-content-block-30 has-negative-sides-30 clearfix\",\n",
    "                                           'class':\"lawyers-who-answered has-no-bottom-margin\",'class': \"lawyer clearfix\"})[0]\n",
    "    profile_id = links_answer_link1['data-profile']\n",
    "    lawyer_info = soup1.findAll(['div','u1'],{'class':\"lawyer-key-info\"})[0]\n",
    "    experience =  str(lawyer_info.time)\n",
    "    a= experience.find('<time datetime=\"P35Y\">')\n",
    "    b=experience.find('years')\n",
    "    years_exp = experience[a+29:b-6]\n",
    "    lawyer_info1 = soup1.findAll(['div','u1','li'],{'class':\"lawyer-key-info\",'class':\"iconed-line text-ellipsis\"})[0]\n",
    "    lawyer_field= str(lawyer_info1.text[7:-12])\n",
    "    len_lf = len(lawyer_field)\n",
    "    lawyer_info2 = soup1.findAll(['div','u1'],{'class':\"lawyer-key-info\"})[0]\n",
    "    b2 = str(lawyer_info2.text)\n",
    "    b22 = lawyer_info2.text.find('Review This Lawyer  Compare Save')\n",
    "    a1= lawyer_info2.text.find(lawyer_field)\n",
    "    state = b2[a1+len_lf+20:b22-6]\n",
    "    \n",
    "    h.write(profile_id+'|')\n",
    "    h.write(lawyer_name+'|')\n",
    "    h.write(lawyer_id_link+'|')\n",
    "    h.write(years_exp+'|')\n",
    "    h.write(lawyer_field+'|')\n",
    "    h.write(state)\n",
    "    h.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://answers.justia.com/?page=1\n",
      "https://answers.justia.com/?page=1\n",
      "https://answers.justia.com/?page=1\n",
      "https://answers.justia.com/?page=1\n",
      "https://lawyers.justia.com/lawyer/terry-lynn-garrett-272882\n",
      "https://answers.justia.com/?page=1\n",
      "https://answers.justia.com/?page=1\n",
      "https://answers.justia.com/?page=1\n",
      "https://answers.justia.com/?page=1\n",
      "https://lawyers.justia.com/lawyer/troy-tyson-1525667\n",
      "https://answers.justia.com/?page=1\n",
      "https://answers.justia.com/?page=1\n",
      "https://answers.justia.com/?page=1\n",
      "https://answers.justia.com/?page=1\n",
      "https://lawyers.justia.com/lawyer/shawna-murray-1525133\n",
      "https://answers.justia.com/?page=1\n",
      "https://answers.justia.com/?page=1\n",
      "https://answers.justia.com/?page=1\n",
      "https://answers.justia.com/?page=1\n",
      "https://lawyers.justia.com/lawyer/timothy-denison-1499047\n",
      "https://answers.justia.com/?page=1\n",
      "https://answers.justia.com/?page=1\n",
      "https://answers.justia.com/?page=1\n",
      "https://answers.justia.com/?page=1\n",
      "https://lawyers.justia.com/lawyer/tim-akpinar-1223027\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,2):\n",
    "    try:\n",
    "        \n",
    "        i_string = str(i)\n",
    "        url = 'https://answers.justia.com/?page=' + i_string\n",
    "        for j in range(0,5):\n",
    "        \n",
    "            questions_extract(url,i,j)\n",
    "            area_and_field(url,i,j)\n",
    "            answers_extract(url,i,j)\n",
    "            lawyer_data(url,i,j)\n",
    "            \n",
    "            \n",
    "        time.sleep(4)\n",
    "        \n",
    "    except:\n",
    "        i = i + 1\n",
    "        i_string = str(i)\n",
    "        url = 'https://answers.justia.com/?page=' + i_string\n",
    "        for j in range(0,5):\n",
    "        \n",
    "            questions_extract(url,i,j)\n",
    "            area_and_field(url,i,j)\n",
    "            answers_extract(url,i,j)\n",
    "            lawyer_data(url,i,j)\n",
    "        time.sleep(4)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()\n",
    "g.close()\n",
    "h.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
